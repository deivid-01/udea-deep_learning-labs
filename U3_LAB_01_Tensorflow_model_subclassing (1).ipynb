{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "p38",
      "language": "python",
      "name": "p38"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "U3_LAB_01_Tensorflow_model_subclassing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1qWNISVzGu7"
      },
      "source": [
        "# LAB 3.1 - TF model subclassing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OWOr9_qzGvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ebec3c-636c-4d54-d3b6-ace15d6938c6"
      },
      "source": [
        "!wget -nc --no-cache -O init.py -q https://raw.githubusercontent.com/rramosp/2021.deeplearning/main/content/init.py\n",
        "import init; init.init(force_download=False); "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replicating local resources\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-QOSo6HZzGvD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe747df1-6897-414e-fbe4-773f37d47e5c"
      },
      "source": [
        "from local.lib.rlxmoocapi import submit, session\n",
        "import inspect\n",
        "student = session.Session(init.endpoint).login( course_id=init.course_id, lab_id=\"L03.01\" )"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "username: davida.torres@udea.edu.co\n",
            "password: ··········\n",
            "using session UDEA\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOnSelfnzGvD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "af8b05b7-30b1-427e-f8ca-ab6d6020408c"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%load_ext tensorboard\n",
        "\n",
        "from sklearn.datasets import *\n",
        "from local.lib import mlutils\n",
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn8QNCO_zGvE"
      },
      "source": [
        "\n",
        "\n",
        "**A multilayer perceptron**\n",
        "\n",
        "assuming $n$ layers, the output at layer $i$\n",
        "\n",
        "$$\\mathbf{a}_i = \\text{activation}(\\mathbf{a}_{i-1} \\cdot \\mathbf{W}_i + \\mathbf{b}_i)$$\n",
        "\n",
        "at the first layer\n",
        "\n",
        "$$\\mathbf{a}_0 = \\text{activation}(\\mathbf{X} \\cdot \\mathbf{W}_0 + \\mathbf{b}_0)$$\n",
        "\n",
        "and the layer prediction is the output of the last layer:\n",
        "\n",
        "$$\\hat{\\mathbf{y}} = \\mathbf{a}_{n-1}$$ \n",
        "\n",
        "with $\\text{activation}$ being an activation function, such as $\\text{sigmoid}(z) = \\frac{1}{1+e^{-z}}$, $\\text{tanh}$, $\\text{ReLU}$, etc.\n",
        "\n",
        "\n",
        "**Cost (with regularization)**\n",
        "\n",
        "\n",
        "$$J(\\mathbf{b}_1, b_2, \\mathbf{W}_1, \\mathbf{W}_2) = \\frac{1}{m}\\sum_{i=0}^{m-1} (\\hat{y}-y)^2 + \\lambda \\sum_{i=0}^{n-1} \\bigg[ \\| \\mathbf{b}_i\\|^2 + \\|\\mathbf{W}_i\\|^2 \\bigg]$$\n",
        "\n",
        "\n",
        "$\\lambda$ regulates the participation of the regularization terms. Given a vector or matrix $\\mathbf{T}$, its squared norm is denoted by $||\\mathbf{T}||^2 \\in \\mathbb{R}$ and it's computed by squaring all its elements and summing them all up. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC0qmDFBzGvE"
      },
      "source": [
        "## TASK 1: Model `build` \n",
        "\n",
        "Observe the class template below which is used to build a multilayer perceptron with a specific number of layers. In the constructor.\n",
        "\n",
        "- `neurons` must be a list of integers specifying the number of neurons of each hidden layer and the output layer.\n",
        "- `activations` must be a list of strings specifying  the activations of the neurons of each layer.\n",
        "\n",
        "Both `neurons` and `activations` must have the same number of elements. Observe how in the class constructor (`__init__`) we check for this and transform the list of activation strings to actual TF funcions.\n",
        "\n",
        "**YOU MUST** complete the `build` method in the class below so that `self.W` and `self.b` contain a list of tensors with randomly initialized weights for each layer. Create the weights by calling the `self.add_weights` function for each layer, both for the weights (add them to list `self.W`) and the biases (add them to list `b`). Call `self.add_weights` with parameters `initializer='random_normal', trainable=True, dtype=tf.float32`.\n",
        "\n",
        "Note that the shape of the first layer weights are not known until the `build` method is called which is when the `input_shape` for the input data is known. For instance, the following invokations\n",
        "\n",
        "\n",
        "    >> mlp = MLP_class(neurons=[10,5,1], activations=[\"tanh\",\"tanh\", \"sigmoid\"])\n",
        "    >> mlp.build([None, 2])\n",
        "    >> print (\"W shapes\", [i.shape for i in mlp.W])\n",
        "    \n",
        "should produce the following output\n",
        "    \n",
        "    W shapes [TensorShape([2, 10]), TensorShape([10, 5]), TensorShape([5, 1])]\n",
        "    b shapes [TensorShape([10]), TensorShape([5]), TensorShape([1])]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYxJ_G_NzGvF"
      },
      "source": [
        "def MLP(neurons, activations, reg=0.):\n",
        "\n",
        "    from tensorflow.keras import Model\n",
        "    from tensorflow.keras.activations import relu, sigmoid, tanh, linear\n",
        "    import numpy as np\n",
        "    import tensorflow as tf    \n",
        "\n",
        "    class MLP_class(Model):\n",
        "        def __init__(self, neurons, activations, reg=0.):\n",
        "            super().__init__()\n",
        "            self.activation_map = {\"linear\": linear, \"relu\": relu, \"tanh\":tanh, \"sigmoid\": sigmoid}\n",
        "            \n",
        "            assert len(neurons)==len(activations), \\\n",
        "                        \"must have the same number of neurons and activations\"\n",
        "                \n",
        "            assert np.alltrue([i in self.activation_map.keys() for i in activations]), \\\n",
        "                                \"activation string not recognized\"\n",
        "            \n",
        "            self.neurons = neurons\n",
        "            self.reg = reg\n",
        "            self.activations = [self.activation_map[i] for i in activations]\n",
        "\n",
        "            super().__init__()\n",
        "\n",
        "        def build(self, input_shape):\n",
        "                    \n",
        "            self.W = []\n",
        "            self.b = []\n",
        "            \n",
        "            for i,neuron in enumerate(self.neurons):\n",
        "\n",
        "              idx = input_shape[-1] if i == 0 else self.neurons[i-1]\n",
        " \n",
        "              layer_weights = self.add_weight(shape=(idx, neuron), initializer='random_normal',\n",
        "                                              trainable=True, dtype=tf.float32)\n",
        "              layer_bias= self.add_weight(shape=(neuron,), initializer='random_normal',\n",
        "                                           trainable=True, dtype=tf.float32)         \n",
        "\n",
        "              self.W.append( layer_weights)\n",
        "              self.b.append( layer_bias)\n",
        "            \n",
        "    return MLP_class(neurons, activations, reg)\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGW_l3R_zGvF"
      },
      "source": [
        "test manually your code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kZMYaoUzGvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc655ee-53f7-48fe-9af4-aa673402803c"
      },
      "source": [
        "mlp = MLP(neurons=[10,5,1], activations=[\"tanh\",\"tanh\", \"sigmoid\"])\n",
        "mlp.build([None, 2])\n",
        "print (\"W shapes\", [i.shape for i in mlp.W])\n",
        "print (\"b shapes\", [i.shape for i in mlp.b])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W shapes [TensorShape([2, 10]), TensorShape([10, 5]), TensorShape([5, 1])]\n",
            "b shapes [TensorShape([10]), TensorShape([5]), TensorShape([1])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP-GrQl4zGvH"
      },
      "source": [
        "**Registra tu solución en linea**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "0S2L7Q1NzGvH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "99cd0777-2c82-49ea-b401-8f2e92747e24"
      },
      "source": [
        "student.submit_task(namespace=globals(), task_id='T1');"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <b>T1 submitted.</b> <b><font color=\"blue\">your grade is 5</font></b> \n",
              "                    <p/><pre>----- grader message -------</pre>testing your code with 100 random calls<br/><b>correct</b><pre>----------------------------</pre>\n",
              "                    <p/><p/>\n",
              "                    <div style=\"font-size:10px\"><b>SUBMISSION CODE</b> /Ou3kkBEv57SSiiA1wHWCeQAOQEAV2oRmVrCfUr7KDZjmRQ23VRBO1CT1FlvtS7RUmIee/NWAjJJ1teCHBbZ6ODkSKgi96mBKw8W+wBfYSmi/5t+ZWSescCPqckvBywRRKAA4tWU0cSOMR6NKyo6iw==</div>\n",
              "                    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzcTd-CgzGvH"
      },
      "source": [
        "## Task 2: Model `call` \n",
        "\n",
        "Complete the `call` method below so that it computes the output of the configured MLP with the input `X` as\n",
        "\n",
        "$$\\hat{\\mathbf{y}} = \\mathbf{a}_{n-1}$$ \n",
        "\n",
        "as described above. Use `self.W`, `self.b` and `self.activations` as constructed previously on the `build` and `__init__` methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyvxCBK7zGvI"
      },
      "source": [
        "def MLP2(neurons, activations, reg=0.):\n",
        "    \n",
        "    from tensorflow.keras import Model\n",
        "    from tensorflow.keras.activations import relu, sigmoid, tanh, linear   \n",
        "\n",
        "    class MLP_class(Model):\n",
        "        def __init__(self, neurons, activations, reg=0.):\n",
        "            super().__init__()\n",
        "            self.activation_map = {\"linear\": linear, \"relu\": relu, \"tanh\":tanh, \"sigmoid\": sigmoid}\n",
        "            \n",
        "            assert len(neurons)==len(activations), \\\n",
        "                        \"must have the same number of neurons and activations\"\n",
        "                \n",
        "            assert np.alltrue([i in self.activation_map.keys() for i in activations]), \\\n",
        "                                \"activation string not recognized\"\n",
        "            \n",
        "            self.neurons = neurons\n",
        "            self.reg = reg\n",
        "            self.activations = [self.activation_map[i] for i in activations]\n",
        "\n",
        "            super().__init__()\n",
        "\n",
        "        def build(self, input_shape):\n",
        "          \n",
        "            self.W = []\n",
        "            self.b = []\n",
        "            \n",
        "            for i,neuron in enumerate(self.neurons):\n",
        "\n",
        "              idx = input_shape[-1] if i == 0 else self.neurons[i-1]\n",
        " \n",
        "              layer_weights = self.add_weight(shape=(idx, neuron), initializer='random_normal',\n",
        "                                              trainable=True, dtype=tf.float32)\n",
        "              layer_bias= self.add_weight(shape=(neuron,), initializer='random_normal',\n",
        "                                           trainable=True, dtype=tf.float32)         \n",
        "\n",
        "              self.W.append( layer_weights)\n",
        "              self.b.append( layer_bias)\n",
        "            \n",
        "        @tf.function\n",
        "        def call(self, X):\n",
        "      \n",
        "          a = self.activations[0](X@self.W[0]+self.b[0])\n",
        "          for  i,activation in  enumerate(self.activations[1:]):\n",
        "              a = activation(a@self.W[i+1]+self.b[i+1]) \n",
        "          return a\n",
        "        \n",
        "    return MLP_class(neurons, activations, reg)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_O2u15izGvI"
      },
      "source": [
        "test manually your code, the following two cells must return the same value everytime you execute them. Observe your MLP will initialize to different random weights each time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91qfsnz8zGvI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f029164-ba5b-4a58-a916-92aa910b0040"
      },
      "source": [
        "X = np.random.random(size=(4,2))\n",
        "neurons = [3,2]\n",
        "mlp = MLP2(neurons=[3,2], activations=[\"linear\", \"sigmoid\"])\n",
        "mlp(X)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
              "array([[0.49994567, 0.50657564],\n",
              "       [0.49993595, 0.50623894],\n",
              "       [0.49896458, 0.5078684 ],\n",
              "       [0.49965987, 0.5069898 ]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no79OhtvzGvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b6061ad-5b64-4fb0-903a-5b9367749cb0"
      },
      "source": [
        "sigm = lambda x: 1/(1+np.exp(-x))\n",
        "W = [i.numpy() for i in mlp.W]\n",
        "b = [i.numpy() for i in mlp.b]\n",
        "sigm((X.dot(W[0])+b[0]).dot(W[1])+b[1])"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.49994567, 0.50657564],\n",
              "       [0.49993596, 0.50623892],\n",
              "       [0.49896458, 0.50786844],\n",
              "       [0.49965986, 0.50698976]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiIH2xM6zGvJ"
      },
      "source": [
        "**Registra tu solución en linea**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "-TL8i7oJzGvJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "d29ed46c-0c20-417d-a527-47a0f2eb0791"
      },
      "source": [
        "student.submit_task(namespace=globals(), task_id='T2');"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <b>T2 submitted.</b> <b><font color=\"blue\">your grade is 5</font></b> \n",
              "                    <p/><pre>----- grader message -------</pre>testing your code with 100 random calls<br/><b>correct</b><pre>----------------------------</pre>\n",
              "                    <p/><p/>\n",
              "                    <div style=\"font-size:10px\"><b>SUBMISSION CODE</b> 0MsVDfLjvzq3IDcsdEJrwnVHXmPK4ajbNZi1jCGTx8xqYbxOQ8RgSBH3BSgpHvzRUXG16bMTv5E2jr+G1bDgt6eiIdHThWSC9ycGyJjhpWdy46HKgpRlXimapyt7uyY/9i9hoRya3nyjjdO8AcZFew==</div>\n",
              "                    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4iK0FtszGvJ"
      },
      "source": [
        "## Task 3: Loss function\n",
        "\n",
        "Complete the `loss` method below so that it computes the loss of the `MLP` given predictions `y_pred` (as the output of the network) and desired output `y_true`.\n",
        "\n",
        "$$J(\\mathbf{b}_1, b_2, \\mathbf{W}_1, \\mathbf{W}_2) = \\frac{1}{m}\\sum_{i=0}^{m-1} (\\hat{y}-y)^2 + \\lambda \\sum_{i=0}^{n-1} \\bigg[ \\| \\mathbf{b}_i\\|^2_{mean} + \\|\\mathbf{W}_i\\|^2_{mean} \\bigg]$$\n",
        "\n",
        "\n",
        "observe the regularization term $\\lambda$ which is stored as `self.reg` in your class.\n",
        "\n",
        "For any weight or bias $\\mathbf{k}$, the expression $\\| \\mathbf{k}\\|^2_{mean}$ is the mean of all its elements squared."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPijABNNzGvK"
      },
      "source": [
        "def MLP3(neurons, activations, reg=0.):\n",
        "    \n",
        "    from tensorflow.keras import Model\n",
        "    from tensorflow.keras.activations import relu, sigmoid, tanh, linear   \n",
        "\n",
        "    class MLP_class(Model):\n",
        "        def __init__(self, neurons, activations, reg=0.):\n",
        "            super().__init__()\n",
        "            self.activation_map = {\"linear\": linear, \"relu\": relu, \"tanh\":tanh, \"sigmoid\": sigmoid}\n",
        "            \n",
        "            assert len(neurons)==len(activations), \\\n",
        "                        \"must have the same number of neurons and activations\"\n",
        "                \n",
        "            assert np.alltrue([i in self.activation_map.keys() for i in activations]), \\\n",
        "                                \"activation string not recognized\"\n",
        "            \n",
        "            self.neurons = neurons\n",
        "            self.reg = reg\n",
        "            self.activations = [self.activation_map[i] for i in activations]\n",
        "\n",
        "            super().__init__()\n",
        "\n",
        "        def build(self, input_shape):\n",
        "            self.W = []\n",
        "            self.b = []\n",
        "            \n",
        "            for i,neuron in enumerate(self.neurons):\n",
        "\n",
        "              idx = input_shape[-1] if i == 0 else self.neurons[i-1]\n",
        " \n",
        "              layer_weights = self.add_weight(shape=(idx, neuron), initializer='random_normal',\n",
        "                                              trainable=True, dtype=tf.float32)\n",
        "              layer_bias= self.add_weight(shape=(neuron,), initializer='random_normal',\n",
        "                                           trainable=True, dtype=tf.float32)         \n",
        "\n",
        "              self.W.append( layer_weights)\n",
        "              self.b.append( layer_bias)\n",
        "            \n",
        "        @tf.function\n",
        "        def call(self, X):\n",
        "              \n",
        "            a = self.activations[0](X@self.W[0]+self.b[0])\n",
        "            for  i,activation in  enumerate(self.activations[1:]):\n",
        "                a = activation(a@self.W[i+1]+self.b[i+1]) \n",
        "            return a\n",
        "        \n",
        "        @tf.function\n",
        "        def loss(self, y_true, y_pred):\n",
        "            return  tf.reduce_mean( (y_pred - y_true)**2 ) + self.reg*tf.reduce_sum( [tf.reduce_mean((i**2)) for i in self.W+self.b])\n",
        "            \n",
        "        \n",
        "    return MLP_class(neurons, activations, reg)"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7olXjC9EzGvK"
      },
      "source": [
        "test manually your code, the following two cells must return the same value everytime you execute them. Observe your MLP will initialize to different random weights each time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlvMJ6QHzGvK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bfa1b2e-0581-4310-fc61-f74277ecd554"
      },
      "source": [
        "X = np.random.random(size=(4,2)).astype(np.float32)\n",
        "y_true = np.random.randint(2, size=(len(X),1)).astype(np.float32)\n",
        "neurons = [3,2]\n",
        "mlp = MLP3(neurons=[3,1], activations=[\"linear\", \"sigmoid\"], reg=0.2)\n",
        "mlp.loss(mlp(X), y_true).numpy()"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25377706"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qf1VJ6ezGvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f541a745-010c-4428-9e96-e12d610f4a22"
      },
      "source": [
        "sigm = lambda x: 1/(1+np.exp(-x))\n",
        "W = [i.numpy() for i in mlp.W]\n",
        "b = [i.numpy() for i in mlp.b]\n",
        "y_pred = sigm((X.dot(W[0])+b[0]).dot(W[1])+b[1])\n",
        "((y_pred-y_true)**2).mean() + mlp.reg * np.sum([(i**2).numpy().mean() for i in mlp.W+mlp.b])"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2537770701572299"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZJ6YBLtzGvL"
      },
      "source": [
        "**Registra tu solución en linea**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "lOLYsfPDzGvL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "54e96815-eb13-4eb8-a8f8-b4e9af1a0de6"
      },
      "source": [
        "student.submit_task(namespace=globals(), task_id='T3');"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <b>T3 submitted.</b> <b><font color=\"blue\">your grade is 5</font></b> \n",
              "                    <p/><pre>----- grader message -------</pre>testing your code with 100 random calls<br/><b>correct</b><pre>----------------------------</pre>\n",
              "                    <p/><p/>\n",
              "                    <div style=\"font-size:10px\"><b>SUBMISSION CODE</b> /O4ZFeye12NWMBV6HWnYcWMGZ7EjKfIwNUsDlLbtP3DbwO5eDCcjDjUgyE4xbiodeaklhQ3ML9XdX2+aiBNugWdCpgjhOguc2hhFbiX4/mABHYCqD1eHGkW7xrBpchzf1oHltfelFzVpDi1d9WwU8g==</div>\n",
              "                    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BugMHeggzGvM"
      },
      "source": [
        "## Done!!\n",
        "\n",
        "now you can try your class with synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcCva7j-zGvM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "00a7032a-eac3-4589-ab8d-38a03783fece"
      },
      "source": [
        "X, y = make_moons(200, noise=.35)\n",
        "X, y = X.astype(np.float32), y.astype(np.float32).reshape(-1,1)\n",
        "plt.scatter(X[:,0][y[:,0]==0], X[:,1][y[:,0]==0], color=\"red\", label=\"class 0\")\n",
        "plt.scatter(X[:,0][y[:,0]==1], X[:,1][y[:,0]==1], color=\"blue\", label=\"class 1\")"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fdffcf648d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yYXB_2szGvM"
      },
      "source": [
        "create MLP and train!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qe_VElzzGvM"
      },
      "source": [
        "mlp = MLP(neurons=[10,1], activations=[\"tanh\",\"sigmoid\"], reg=0.0)\n",
        "mlp.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss=mlp.loss,\n",
        "           metrics=[tf.keras.metrics.mae, tf.keras.metrics.binary_accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQoWLCUszGvM"
      },
      "source": [
        "!rm -rf logs\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/no_regularization\")\n",
        "mlp.fit(X,y, epochs=400, batch_size=16, verbose=0, \n",
        "        callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvQKzI7HzGvN"
      },
      "source": [
        "observe the accuracy and classification frontier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfD_gosBzGvN"
      },
      "source": [
        "predict = lambda X: (mlp.predict(X)[:,0]>0.5).astype(int)\n",
        "mlutils.plot_2Ddata_with_boundary(predict, X, y.reshape(-1));\n",
        "plt.title(\"accuracy %.2f\"%np.mean(predict(X)==y.reshape(-1)));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1mc2Y_nzGvN"
      },
      "source": [
        "regularization must work!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6noAgKZOzGvN"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/with_regularization\")\n",
        "mlp = MLP(neurons=[10,1], activations=[\"tanh\",\"sigmoid\"], reg=0.005)\n",
        "\n",
        "mlp.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss=mlp.loss,\n",
        "           metrics=[tf.keras.metrics.mae, tf.keras.metrics.binary_accuracy])\n",
        "\n",
        "mlp.fit(X,y, epochs=400, batch_size=10, verbose=0, callbacks=[tensorboard_callback])\n",
        "mlutils.plot_2Ddata_with_boundary(predict, X, y.reshape(-1))\n",
        "plt.title(\"accuracy %.2f\"%np.mean(predict(X)==y.reshape(-1)));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpZsRwTuzGvN"
      },
      "source": [
        "and inspect tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdZ8bQFKzGvO"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mon59yENzGvO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}